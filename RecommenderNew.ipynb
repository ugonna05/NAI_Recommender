{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540000eb",
   "metadata": {},
   "source": [
    "building hybrid recommender system for our blockchain usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cfb34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9599d7f7-cbd5-4a90-8318-41cd01d45fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.preprocessing import OrdinalEncoder\\n\\nself.user_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "self.user_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0cd9b41-d2d8-4e31-b503-137a164e1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_transform(encoder, values):\n",
    "    known = set(encoder.classes_)\n",
    "    return [\n",
    "        encoder.transform([v])[0] if v in known else -1\n",
    "        for v in values\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "067ec1bc-fedf-4044-b0b0-34c4b5a72e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'self.users_df[\\'user_id_encoded\\'] = safe_transform(self.user_encoder, self.users_df[\\'user_id\"])'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"self.users_df['user_id_encoded'] = safe_transform(self.user_encoder, self.users_df['user_id\"])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28fe981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockchainDataset(Dataset):\n",
    "    def __init__(self, interactions_df, users_df, assets_df, asset_graph_df, user_graph_df):\n",
    "        self.interactions_df = interactions_df\n",
    "        self.users_df = users_df\n",
    "        self.assets_df = assets_df\n",
    "        self.asset_graph_df = asset_graph_df\n",
    "        self.user_graph_df = user_graph_df\n",
    "        \n",
    "        # Prepare data\n",
    "        self.prepare_data()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        # Encode user and asset IDs\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.asset_encoder = LabelEncoder()\n",
    "        \n",
    "        self.interactions_df['user_id_encoded'] = self.user_encoder.fit_transform(\n",
    "            self.interactions_df['user_id']\n",
    "        )\n",
    "        self.interactions_df['asset_id_encoded'] = self.asset_encoder.fit_transform(\n",
    "            self.interactions_df['asset_id']\n",
    "        )\n",
    "        # Prepare user features\n",
    "        self.users_df['user_id_encoded'] = safe_transform(self.user_encoder, self.users_df['user_id']\n",
    "        )\n",
    "        \n",
    "        # Prepare asset features\n",
    "        self.assets_df['asset_id_encoded'] = self.asset_encoder.transform(\n",
    "            self.assets_df['asset_id']\n",
    "        )\n",
    "        \n",
    "        # Create user features matrix\n",
    "        user_numeric_cols = ['wallet_age_days', 'tx_count', 'avg_gas_used', \n",
    "                           'portfolio_diversity', 'risk_score']\n",
    "        self.user_features_scaler = StandardScaler()\n",
    "        user_numeric_features = self.users_df[user_numeric_cols].values\n",
    "        self.user_features_scaled = self.user_features_scaler.fit_transform(user_numeric_features)\n",
    "        \n",
    "        # One-hot encode preferred_network\n",
    "        network_dummies = pd.get_dummies(self.users_df['preferred_network'], prefix='network')\n",
    "        self.user_features = np.hstack([self.user_features_scaled, network_dummies.values])\n",
    "        \n",
    "        # Create asset features matrix\n",
    "        asset_numeric_cols = ['price', 'volatility', 'tvl', 'audit_score', \n",
    "                            'github_commits', 'social_sentiment']\n",
    "        self.asset_features_scaler = StandardScaler()\n",
    "        asset_numeric_features = self.assets_df[asset_numeric_cols].values\n",
    "        self.asset_features_scaled = self.asset_features_scaler.fit_transform(asset_numeric_features)\n",
    "        \n",
    "        # One-hot encode asset_type\n",
    "        asset_type_dummies = pd.get_dummies(self.assets_df['asset_type'], prefix='type')\n",
    "        self.asset_features = np.hstack([self.asset_features_scaled, asset_type_dummies.values])\n",
    "        \n",
    "        # Build asset graph features\n",
    "        self.asset_graph = self.build_asset_graph_features()\n",
    "        \n",
    "        # Build user graph features\n",
    "        self.user_graph = self.build_user_graph_features()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b42acb60-a407-4b2d-bb29-7719ce77524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_asset_graph_features(self):\n",
    "        \"\"\"Build asset graph and extract graph-based features\"\"\"\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for asset in self.assets_df['asset_id']:\n",
    "            G.add_node(asset)\n",
    "        \n",
    "        # Add edges with weights\n",
    "        for _, row in self.asset_graph_df.iterrows():\n",
    "            G.add_edge(row['asset1'], row['asset2'], \n",
    "                      weight=row['strength'], \n",
    "                      relation_type=row['relation_type'])\n",
    "        \n",
    "        # Calculate graph metrics for each asset\n",
    "        asset_metrics = {}\n",
    "        for asset in self.assets_df['asset_id']:\n",
    "            if asset in G:\n",
    "                try:\n",
    "                    degree = G.degree(asset)\n",
    "                    clustering = nx.clustering(G, asset)\n",
    "                    \n",
    "                    # Get relation type distribution\n",
    "                    relation_types = {}\n",
    "                    for neighbor in G.neighbors(asset):\n",
    "                        edge_data = G.get_edge_data(asset, neighbor)\n",
    "                        rel_type = edge_data.get('relation_type', 'unknown')\n",
    "                        relation_types[rel_type] = relation_types.get(rel_type, 0) + 1\n",
    "                    \n",
    "                    asset_metrics[asset] = {\n",
    "                        'degree': degree,\n",
    "                        'clustering': clustering,\n",
    "                        'paired_lp_connections': relation_types.get('paired_lp', 0),\n",
    "                        'bridge_connections': relation_types.get('bridge', 0),\n",
    "                        'governance_connections': relation_types.get('governance', 0)\n",
    "                    }\n",
    "                except:\n",
    "                    asset_metrics[asset] = {\n",
    "                        'degree': 0, 'clustering': 0,\n",
    "                        'paired_lp_connections': 0, 'bridge_connections': 0, \n",
    "                        'governance_connections': 0\n",
    "                    }\n",
    "            else:\n",
    "                asset_metrics[asset] = {\n",
    "                    'degree': 0, 'clustering': 0,\n",
    "                    'paired_lp_connections': 0, 'bridge_connections': 0, \n",
    "                    'governance_connections': 0\n",
    "                }\n",
    "        \n",
    "        # Add graph metrics to asset features\n",
    "        graph_features = []\n",
    "        for asset in self.assets_df['asset_id']:\n",
    "            metrics = asset_metrics[asset]\n",
    "            graph_features.append([\n",
    "                metrics['degree'],\n",
    "                metrics['clustering'],\n",
    "                metrics['paired_lp_connections'],\n",
    "                metrics['bridge_connections'],\n",
    "                metrics['governance_connections']\n",
    "            ])\n",
    "        \n",
    "        return np.array(graph_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8e3b24c-fffa-44d3-bd65-abbf619fbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_graph_features(self):\n",
    "        \"\"\"Build user graph and extract graph-based features\"\"\"\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for user in self.users_df['user_id']:\n",
    "            G.add_node(user)\n",
    "        \n",
    "        # Add edges\n",
    "        for _, row in self.user_graph_df.iterrows():\n",
    "            G.add_edge(row['user1'], row['user2'], \n",
    "                      relation_type=row['relation_type'])\n",
    "        \n",
    "        # Calculate graph metrics for each user\n",
    "        user_metrics = {}\n",
    "        for user in self.users_df['user_id']:\n",
    "            if user in G:\n",
    "                try:\n",
    "                    degree = G.degree(user)\n",
    "                    clustering = nx.clustering(G, user)\n",
    "                    \n",
    "                    # Get relation type distribution\n",
    "                    follows_count = 0\n",
    "                    co_tx_count = 0\n",
    "                    for neighbor in G.neighbors(user):\n",
    "                        edge_data = G.get_edge_data(user, neighbor)\n",
    "                        rel_type = edge_data.get('relation_type', 'unknown')\n",
    "                        if rel_type == 'follows':\n",
    "                            follows_count += 1\n",
    "                        elif rel_type == 'co_tx':\n",
    "                            co_tx_count += 1\n",
    "                    \n",
    "                    user_metrics[user] = {\n",
    "                        'degree': degree,\n",
    "                        'clustering': clustering,\n",
    "                        'follows_count': follows_count,\n",
    "                        'co_tx_count': co_tx_count\n",
    "                    }\n",
    "                except:\n",
    "                    user_metrics[user] = {\n",
    "                        'degree': 0, 'clustering': 0,\n",
    "                        'follows_count': 0, 'co_tx_count': 0\n",
    "                    }\n",
    "            else:\n",
    "                user_metrics[user] = {\n",
    "                    'degree': 0, 'clustering': 0,\n",
    "                    'follows_count': 0, 'co_tx_count': 0\n",
    "                }\n",
    "        \n",
    "        # Add graph metrics to user features\n",
    "        graph_features = []\n",
    "        for user in self.users_df['user_id']:\n",
    "            metrics = user_metrics[user]\n",
    "            graph_features.append([\n",
    "                metrics['degree'],\n",
    "                metrics['clustering'],\n",
    "                metrics['follows_count'],\n",
    "                metrics['co_tx_count']\n",
    "            ])\n",
    "        \n",
    "        return np.array(graph_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17ef597f-77f3-4d0d-b27b-4e90ae9e1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "        return len(self.interactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12614b30-c070-4e06-bfe3-7774b6a9a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    row = self.interactions_df.iloc[idx]\n",
    "        \n",
    "    user_id = row['user_id_encoded']\n",
    "    asset_id = row['asset_id_encoded']\n",
    "    rating = row['rating'] / 5.0  # Normalize to [0,1]\n",
    "        \n",
    "    # Get enhanced user features (basic + graph)\n",
    "    user_basic_features = self.user_features[user_id]\n",
    "    user_graph_features = self.user_graph[user_id]\n",
    "    user_combined_features = np.concatenate([user_basic_features, user_graph_features])\n",
    "        \n",
    "    # Get enhanced asset features (basic + graph)\n",
    "    asset_basic_features = self.asset_features[asset_id]\n",
    "    asset_graph_features = self.asset_graph[asset_id]\n",
    "    asset_combined_features = np.concatenate([asset_basic_features, asset_graph_features])\n",
    "        \n",
    "    return (\n",
    "        torch.LongTensor([user_id]),\n",
    "        torch.LongTensor([asset_id]),\n",
    "        torch.FloatTensor(user_combined_features),\n",
    "        torch.FloatTensor(asset_combined_features),\n",
    "        torch.FloatTensor([rating])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea29de38-9a12-4324-bd2d-94dbd5224782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEnhancedHybridRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, user_features_dim, item_features_dim,\n",
    "                 embedding_dim=64, hidden_dim=128, dropout_rate=0.3):\n",
    "        super(GraphEnhancedHybridRecommender, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Feature encoders with graph-enhanced dimensions\n",
    "        self.user_encoder = nn.Sequential(\n",
    "            nn.Linear(user_features_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.item_encoder = nn.Sequential(\n",
    "            nn.Linear(item_features_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads=4, dropout=dropout_rate)\n",
    "        \n",
    "        # Fusion network\n",
    "        self.fusion_network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 4, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.user_bn = nn.BatchNorm1d(embedding_dim)\n",
    "        self.item_bn = nn.BatchNorm1d(embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self._init_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aabcedb0-b09d-4a20-8b1f-0a78c66db5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        \n",
    "        for layer in [self.user_encoder, self.item_encoder, self.fusion_network]:\n",
    "            if isinstance(layer, nn.Sequential):\n",
    "                for sublayer in layer:\n",
    "                    if isinstance(sublayer, nn.Linear):\n",
    "                        nn.init.xavier_uniform_(sublayer.weight)\n",
    "                        nn.init.zeros_(sublayer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8e1ea5b-21fd-4e73-8f66-5408541cace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, user_ids, item_ids, user_features, item_features):\n",
    "        # Collaborative filtering embeddings\n",
    "        user_embed = self.user_embedding(user_ids).squeeze(1)\n",
    "        item_embed = self.item_embedding(item_ids).squeeze(1)\n",
    "        \n",
    "        # Content-based features\n",
    "        user_content = self.user_encoder(user_features)\n",
    "        item_content = self.item_encoder(item_features)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        user_embed = self.user_bn(user_embed)\n",
    "        item_embed = self.item_bn(item_embed)\n",
    "        user_content = self.user_bn(user_content)\n",
    "        item_content = self.item_bn(item_content)\n",
    "        \n",
    "        # Apply attention\n",
    "        user_combined = torch.stack([user_embed, user_content], dim=0)\n",
    "        item_combined = torch.stack([item_embed, item_content], dim=0)\n",
    "        \n",
    "        user_attended, _ = self.attention(user_combined, user_combined, user_combined)\n",
    "        item_attended, _ = self.attention(item_combined, item_combined, item_combined)\n",
    "        \n",
    "        user_attended = user_attended.mean(dim=0)\n",
    "        item_attended = item_attended.mean(dim=0)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined = torch.cat([user_attended, item_attended, user_embed, item_embed], dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.fusion_network(combined)\n",
    "        return torch.sigmoid(prediction.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3099679-b521-42e0-acfd-c1d7b86b75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedColdStartHandler:\n",
    "    def __init__(self, n_clusters=5, n_neighbors=10):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.user_cluster_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.item_cluster_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.user_knn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        self.item_knn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        self.is_fitted = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d5b6f8b-522e-4919-bc22-4c46b07901e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, user_features, item_features, interactions_df):\n",
    "        self.user_cluster_model.fit(user_features)\n",
    "        self.item_cluster_model.fit(item_features)\n",
    "        self.user_knn.fit(user_features)\n",
    "        self.item_knn.fit(item_features)\n",
    "        \n",
    "        # Create user-item matrix for cold start recommendations\n",
    "        self.user_item_matrix = interactions_df.pivot(\n",
    "            index='user_id_encoded', \n",
    "            columns='asset_id_encoded', \n",
    "            values='rating'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e83e6bb0-cc5d-45f7-8949-90e1a30ea5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_similar_users(self, user_features, n_neighbors=5):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"ColdStartHandler must be fitted first\")\n",
    "        distances, indices = self.user_knn.kneighbors(user_features.reshape(1, -1), n_neighbors=n_neighbors)\n",
    "        return indices[0], distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b84bd4b-d37e-4ce3-8844-60e2c4397949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_items(self, item_features, n_neighbors=5):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"ColdStartHandler must be fitted first\")\n",
    "        distances, indices = self.item_knn.kneighbors(item_features.reshape(1, -1), n_neighbors=n_neighbors)\n",
    "        return indices[0], distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b433c881-2987-464b-9ae3-434485e72a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    " def recommend_for_cold_start_user(self, new_user_features, n_recommendations=10):\n",
    "        \"\"\"Recommend for new user based on similar users' preferences\"\"\"\n",
    "        similar_users, _ = self.get_similar_users(new_user_features)\n",
    "        \n",
    "        # Get items liked by similar users\n",
    "        similar_users_ratings = self.user_item_matrix.iloc[similar_users]\n",
    "        item_scores = similar_users_ratings.mean(axis=0)\n",
    "        \n",
    "        # Get top items\n",
    "        top_items = item_scores.nlargest(n_recommendations)\n",
    "        return top_items.index.values, top_items.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60d5c9ca-5482-45d5-872e-be30f8a64283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_cold_start_item(self, new_item_features, n_recommendations=10):\n",
    "        \"\"\"Recommend new item to users who like similar items\"\"\"\n",
    "        similar_items, _ = self.get_similar_items(new_item_features)\n",
    "        \n",
    "        # Get users who liked similar items\n",
    "        similar_items_ratings = self.user_item_matrix.iloc[:, similar_items]\n",
    "        user_scores = similar_items_ratings.mean(axis=1)\n",
    "        \n",
    "        # Get top users\n",
    "        top_users = user_scores.nlargest(n_recommendations)\n",
    "        return top_users.index.values, top_users.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "091ac97d-78dd-4a97-ae8e-341c25f1a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockchainHybridRecommender:\n",
    "    def __init__(self, interactions_df, users_df, assets_df, asset_graph_df, user_graph_df):\n",
    "        self.dataset = BlockchainDataset(interactions_df, users_df, assets_df, asset_graph_df, user_graph_df)\n",
    "        \n",
    "        # Model dimensions\n",
    "        self.num_users = len(users_df)\n",
    "        self.num_items = len(assets_df)\n",
    "        self.user_features_dim = self.dataset.user_features.shape[1] + self.dataset.user_graph.shape[1]\n",
    "        self.item_features_dim = self.dataset.asset_features.shape[1] + self.dataset.asset_graph.shape[1]\n",
    "        \n",
    "        # Initialize model and cold start handler\n",
    "        self.model = GraphEnhancedHybridRecommender(\n",
    "            self.num_users, self.num_items, \n",
    "            self.user_features_dim, self.item_features_dim\n",
    "        )\n",
    "        \n",
    "        self.cold_start_handler = EnhancedColdStartHandler()\n",
    "        \n",
    "        # Optimizer and loss\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        self.criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24315439-61b7-494a-a4e0-07b08cc83345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cold_start_handler(self):\n",
    "        \"\"\"Prepare cold start handler with current data\"\"\"\n",
    "        self.cold_start_handler.fit(\n",
    "            self.dataset.user_features, \n",
    "            self.dataset.asset_features,\n",
    "            self.dataset.interactions_df\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "085f3378-d74e-482c-a507-d35285ec72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, epochs=15, batch_size=64, validation_split=0.2):\n",
    "        \"\"\"Train the recommendation model\"\"\"\n",
    "        # Split data\n",
    "        dataset_size = len(self.dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        split = int(np.floor(validation_split * dataset_size))\n",
    "        np.random.shuffle(indices)\n",
    "        train_indices, val_indices = indices[split:], indices[:split]\n",
    "        \n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "        \n",
    "        train_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = DataLoader(self.dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "        \n",
    "        # Prepare cold start handler\n",
    "        self.prepare_cold_start_handler()\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch_idx, (user_ids, item_ids, user_features, item_features, ratings) in enumerate(train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                predictions = self.model(user_ids, item_ids, user_features, item_features)\n",
    "                loss = self.criterion(predictions, ratings)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{epochs} | Batch {batch_idx} | Loss: {loss.item():.6f}')\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self.validate(val_loader)\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f'Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.6f} | Val Loss: {val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4251d83-b45a-4b56-96be-98346c6104f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for user_ids, item_ids, user_features, item_features, ratings in val_loader:\n",
    "                predictions = self.model(user_ids, item_ids, user_features, item_features)\n",
    "                loss = self.criterion(predictions, ratings)\n",
    "                total_loss += loss.item()\n",
    "        self.model.train()\n",
    "        return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c07b3cc1-5359-4827-b378-a4ba76519c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, user_id, asset_id):\n",
    "        \"\"\"Predict rating for specific user-asset pair\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        user_encoded = self.dataset.user_encoder.transform([user_id])[0]\n",
    "        asset_encoded = self.dataset.asset_encoder.transform([asset_id])[0]\n",
    "        \n",
    "        user_features = torch.FloatTensor(\n",
    "            np.concatenate([\n",
    "                self.dataset.user_features[user_encoded],\n",
    "                self.dataset.user_graph[user_encoded]\n",
    "            ])\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        asset_features = torch.FloatTensor(\n",
    "            np.concatenate([\n",
    "                self.dataset.asset_features[asset_encoded],\n",
    "                self.dataset.asset_graph[asset_encoded]\n",
    "            ])\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(\n",
    "                torch.LongTensor([user_encoded]),\n",
    "                torch.LongTensor([asset_encoded]),\n",
    "                user_features,\n",
    "                asset_features\n",
    "            )\n",
    "        \n",
    "        return prediction.item() * 5.0  # Convert back to original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f71795af-848f-4c5c-ac3e-d146497521d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_top_n(self, user_id, n=10):\n",
    "        \"\"\"Generate top-N recommendations for a user\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        user_encoded = self.dataset.user_encoder.transform([user_id])[0]\n",
    "        \n",
    "        user_features = torch.FloatTensor(\n",
    "            np.concatenate([\n",
    "                self.dataset.user_features[user_encoded],\n",
    "                self.dataset.user_graph[user_encoded]\n",
    "            ])\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        all_scores = []\n",
    "        all_assets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for asset_id in self.dataset.assets_df['asset_id']:\n",
    "                asset_encoded = self.dataset.asset_encoder.transform([asset_id])[0]\n",
    "                \n",
    "                asset_features = torch.FloatTensor(\n",
    "                    np.concatenate([\n",
    "                        self.dataset.asset_features[asset_encoded],\n",
    "                        self.dataset.asset_graph[asset_encoded]\n",
    "                    ])\n",
    "                ).unsqueeze(0)\n",
    "                \n",
    "                score = self.model(\n",
    "                    torch.LongTensor([user_encoded]),\n",
    "                    torch.LongTensor([asset_encoded]),\n",
    "                    user_features,\n",
    "                    asset_features\n",
    "                )\n",
    "                all_scores.append(score.item())\n",
    "                all_assets.append(asset_id)\n",
    "\n",
    " # Get top-N recommendations\n",
    "        recommendations = sorted(zip(all_assets, all_scores), key=lambda x: x[1], reverse=True)[:n]\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb574d50-f248-45a2-b9dc-eb5bf412de29",
   "metadata": {},
   "outputs": [],
   "source": [
    " def handle_cold_start_user(self, user_features, n_recommendations=10):\n",
    "        \"\"\"Handle recommendations for new user (cold start)\"\"\"\n",
    "        recommended_items, scores = self.cold_start_handler.recommend_for_cold_start_user(\n",
    "            user_features, n_recommendations\n",
    "        )\n",
    "        \n",
    "        # Convert encoded item IDs back to original asset IDs\n",
    "        asset_ids = self.dataset.asset_encoder.inverse_transform(recommended_items)\n",
    "        return list(zip(asset_ids, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b881037-b74e-46e2-9b77-904eb03e9025",
   "metadata": {},
   "outputs": [],
   "source": [
    " def handle_cold_start_item(self, item_features, n_recommendations=10):\n",
    "        \"\"\"Handle recommendations for new item (cold start)\"\"\"\n",
    "        recommended_users, scores = self.cold_start_handler.recommend_for_cold_start_item(\n",
    "            item_features, n_recommendations\n",
    "        )\n",
    "        \n",
    "        # Convert encoded user IDs back to original user IDs\n",
    "        user_ids = self.dataset.user_encoder.inverse_transform(recommended_users)\n",
    "        return list(zip(user_ids, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac2a7ac1-7da0-41c0-8425-a49dad8e7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Interactions: (500, 6)\n",
      "Users: (100, 7)\n",
      "Assets: (20, 8)\n",
      "Asset Graph: (100, 4)\n",
      "User Graph: (200, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BlockchainDataset' object has no attribute 'build_asset_graph_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted rating for U1-ASSET1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_prediction\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 54\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[51], line 18\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser Graph: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_graph_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize recommender system\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m recommender \u001b[38;5;241m=\u001b[39m BlockchainHybridRecommender(\n\u001b[0;32m     19\u001b[0m     interactions_df, users_df, assets_df, asset_graph_df, user_graph_df\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel initialized with:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecommender\u001b[38;5;241m.\u001b[39mnum_users\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Items: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecommender\u001b[38;5;241m.\u001b[39mnum_items\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m, in \u001b[0;36mBlockchainHybridRecommender.__init__\u001b[1;34m(self, interactions_df, users_df, assets_df, asset_graph_df, user_graph_df)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, interactions_df, users_df, assets_df, asset_graph_df, user_graph_df):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m BlockchainDataset(interactions_df, users_df, assets_df, asset_graph_df, user_graph_df)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Model dimensions\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_users \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(users_df)\n",
      "Cell \u001b[1;32mIn[29], line 10\u001b[0m, in \u001b[0;36mBlockchainDataset.__init__\u001b[1;34m(self, interactions_df, users_df, assets_df, asset_graph_df, user_graph_df)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_graph_df \u001b[38;5;241m=\u001b[39m user_graph_df\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data()\n",
      "Cell \u001b[1;32mIn[29], line 55\u001b[0m, in \u001b[0;36mBlockchainDataset.prepare_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masset_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masset_features_scaled, asset_type_dummies\u001b[38;5;241m.\u001b[39mvalues])\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Build asset graph features\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masset_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_asset_graph_features()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Build user graph features\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_user_graph_features()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BlockchainDataset' object has no attribute 'build_asset_graph_features'"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Load your datasets\n",
    "    interactions_df = pd.read_csv('interactions.csv')\n",
    "    users_df = pd.read_csv('users.csv')\n",
    "    assets_df = pd.read_csv('assets.csv')\n",
    "    asset_graph_df = pd.read_csv('asset_graph.csv')\n",
    "    user_graph_df = pd.read_csv('user_graph.csv')\n",
    "    \n",
    "    print(\"Dataset shapes:\")\n",
    "    print(f\"Interactions: {interactions_df.shape}\")\n",
    "    print(f\"Users: {users_df.shape}\")\n",
    "    print(f\"Assets: {assets_df.shape}\")\n",
    "    print(f\"Asset Graph: {asset_graph_df.shape}\")\n",
    "    print(f\"User Graph: {user_graph_df.shape}\")\n",
    "    \n",
    "    # Initialize recommender system\n",
    "    recommender = BlockchainHybridRecommender(\n",
    "        interactions_df, users_df, assets_df, asset_graph_df, user_graph_df\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel initialized with:\")\n",
    "    print(f\"Users: {recommender.num_users}, Items: {recommender.num_items}\")\n",
    "    print(f\"User features dim: {recommender.user_features_dim}\")\n",
    "    print(f\"Item features dim: {recommender.item_features_dim}\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nTraining model...\")\n",
    "    recommender.train(epochs=10, batch_size=32)\n",
    "    \n",
    "    # Test recommendations for existing user\n",
    "    print(\"\\n=== Testing Recommendations ===\")\n",
    "    test_user = \"U1\"\n",
    "    recommendations = recommender.recommend_top_n(test_user, n=5)\n",
    "    print(f\"Top 5 recommendations for user {test_user}:\")\n",
    "    for asset_id, score in recommendations:\n",
    "        print(f\"  {asset_id}: {score:.4f}\")\n",
    "    \n",
    "    # Test cold start user\n",
    "    print(\"\\n=== Testing Cold Start ===\")\n",
    "    # Create synthetic new user features\n",
    "    new_user_features = np.random.randn(recommender.user_features_dim)\n",
    "    cold_start_recommendations = recommender.handle_cold_start_user(new_user_features, n=5)\n",
    "    print(\"Cold start recommendations for new user:\")\n",
    "    for asset_id, score in cold_start_recommendations:\n",
    "        print(f\"  {asset_id}: {score:.4f}\")\n",
    "    \n",
    "    # Test prediction for specific user-asset pair\n",
    "    print(\"\\n=== Testing Specific Prediction ===\")\n",
    "    test_prediction = recommender.predict(\"U1\", \"ASSET1\")\n",
    "    print(f\"Predicted rating for U1-ASSET1: {test_prediction:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f548ee1-81f7-4ed0-aa91-d08f235b4312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e88e048b-dd3b-4721-8195-506046b59792",
   "metadata": {},
   "source": [
    "I'll enhance the recommender system to handle unseen data through dynamic feature processing and online learning capabilities. Here's the comprehensive solution:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DynamicFeatureProcessor:\n",
    "    \"\"\"Process features dynamically for unseen data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_scaler = StandardScaler()\n",
    "        self.asset_scaler = StandardScaler()\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.asset_encoder = LabelEncoder()\n",
    "        self.network_encoder = LabelEncoder()\n",
    "        self.asset_type_encoder = LabelEncoder()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, users_df, assets_df):\n",
    "        \"\"\"Fit encoders and scalers on initial data\"\"\"\n",
    "        # User features\n",
    "        user_numeric_cols = ['wallet_age_days', 'tx_count', 'avg_gas_used', \n",
    "                           'portfolio_diversity', 'risk_score']\n",
    "        user_numeric = users_df[user_numeric_cols].values\n",
    "        self.user_scaler.fit(user_numeric)\n",
    "        \n",
    "        # Asset features\n",
    "        asset_numeric_cols = ['price', 'volatility', 'tvl', 'audit_score', \n",
    "                            'github_commits', 'social_sentiment']\n",
    "        asset_numeric = assets_df[asset_numeric_cols].values\n",
    "        self.asset_scaler.fit(asset_numeric)\n",
    "        \n",
    "        # Categorical encoders\n",
    "        self.user_encoder.fit(users_df['user_id'])\n",
    "        self.asset_encoder.fit(assets_df['asset_id'])\n",
    "        self.network_encoder.fit(users_df['preferred_network'])\n",
    "        self.asset_type_encoder.fit(assets_df['asset_type'])\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "    def transform_user(self, user_data):\n",
    "        \"\"\"Transform user data (can handle unseen users)\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"FeatureProcessor must be fitted first\")\n",
    "            \n",
    "        # Handle numeric features\n",
    "        numeric_cols = ['wallet_age_days', 'tx_count', 'avg_gas_used', \n",
    "                       'portfolio_diversity', 'risk_score']\n",
    "        numeric_features = self.user_scaler.transform([user_data[col] for col in numeric_cols])\n",
    "        \n",
    "        # Handle categorical features\n",
    "        try:\n",
    "            network_encoded = self.network_encoder.transform([user_data['preferred_network']])[0]\n",
    "        except ValueError:\n",
    "            # Unseen network - assign to most common\n",
    "            network_encoded = 0\n",
    "        \n",
    "        # One-hot encode network\n",
    "        network_onehot = np.zeros(len(self.network_encoder.classes_))\n",
    "        if network_encoded < len(network_onehot):\n",
    "            network_onehot[network_encoded] = 1\n",
    "        \n",
    "        return np.concatenate([numeric_features.flatten(), network_onehot])\n",
    "    \n",
    "    def transform_asset(self, asset_data):\n",
    "        \"\"\"Transform asset data (can handle unseen assets)\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"FeatureProcessor must be fitted first\")\n",
    "            \n",
    "        # Handle numeric features\n",
    "        numeric_cols = ['price', 'volatility', 'tvl', 'audit_score', \n",
    "                       'github_commits', 'social_sentiment']\n",
    "        numeric_features = self.asset_scaler.transform([[asset_data[col] for col in numeric_cols]])\n",
    "        \n",
    "        # Handle categorical features\n",
    "        try:\n",
    "            asset_type_encoded = self.asset_type_encoder.transform([asset_data['asset_type']])[0]\n",
    "        except ValueError:\n",
    "            # Unseen asset type - assign to most common\n",
    "            asset_type_encoded = 0\n",
    "        \n",
    "        # One-hot encode asset type\n",
    "        asset_type_onehot = np.zeros(len(self.asset_type_encoder.classes_))\n",
    "        if asset_type_encoded < len(asset_type_onehot):\n",
    "            asset_type_onehot[asset_type_encoded] = 1\n",
    "        \n",
    "        return np.concatenate([numeric_features.flatten(), asset_type_onehot])\n",
    "    \n",
    "    def get_user_id_encoded(self, user_id):\n",
    "        \"\"\"Get encoded user ID, handling unseen users\"\"\"\n",
    "        try:\n",
    "            return self.user_encoder.transform([user_id])[0]\n",
    "        except ValueError:\n",
    "            # Return a placeholder ID for unseen users\n",
    "            return len(self.user_encoder.classes_)\n",
    "    \n",
    "    def get_asset_id_encoded(self, asset_id):\n",
    "        \"\"\"Get encoded asset ID, handling unseen assets\"\"\"\n",
    "        try:\n",
    "            return self.asset_encoder.transform([asset_id])[0]\n",
    "        except ValueError:\n",
    "            # Return a placeholder ID for unseen assets\n",
    "            return len(self.asset_encoder.classes_)\n",
    "\n",
    "class DynamicGraphBuilder:\n",
    "    \"\"\"Build and update graphs dynamically\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.asset_graph = nx.Graph()\n",
    "        self.user_graph = nx.Graph()\n",
    "        \n",
    "    def update_asset_graph(self, asset_graph_df):\n",
    "        \"\"\"Update asset graph with new relationships\"\"\"\n",
    "        for _, row in asset_graph_df.iterrows():\n",
    "            self.asset_graph.add_edge(\n",
    "                row['asset1'], row['asset2'],\n",
    "                weight=row['strength'],\n",
    "                relation_type=row['relation_type']\n",
    "            )\n",
    "    \n",
    "    def update_user_graph(self, user_graph_df):\n",
    "        \"\"\"Update user graph with new relationships\"\"\"\n",
    "        for _, row in user_graph_df.iterrows():\n",
    "            self.user_graph.add_edge(\n",
    "                row['user1'], row['user2'],\n",
    "                relation_type=row['relation_type']\n",
    "            )\n",
    "    \n",
    "    def get_asset_graph_features(self, asset_id):\n",
    "        \"\"\"Get graph features for an asset (handles unseen assets)\"\"\"\n",
    "        if asset_id not in self.asset_graph:\n",
    "            return np.array([0, 0, 0, 0, 0])  # Default features for unseen assets\n",
    "        \n",
    "        try:\n",
    "            degree = self.asset_graph.degree(asset_id)\n",
    "            clustering = nx.clustering(self.asset_graph, asset_id)\n",
    "            \n",
    "            # Count relation types\n",
    "            relation_counts = {'paired_lp': 0, 'bridge': 0, 'governance': 0}\n",
    "            for neighbor in self.asset_graph.neighbors(asset_id):\n",
    "                edge_data = self.asset_graph.get_edge_data(asset_id, neighbor)\n",
    "                rel_type = edge_data.get('relation_type', 'unknown')\n",
    "                if rel_type in relation_counts:\n",
    "                    relation_counts[rel_type] += 1\n",
    "            \n",
    "            return np.array([\n",
    "                degree,\n",
    "                clustering,\n",
    "                relation_counts['paired_lp'],\n",
    "                relation_counts['bridge'],\n",
    "                relation_counts['governance']\n",
    "            ])\n",
    "        except:\n",
    "            return np.array([0, 0, 0, 0, 0])\n",
    "    \n",
    "    def get_user_graph_features(self, user_id):\n",
    "        \"\"\"Get graph features for a user (handles unseen users)\"\"\"\n",
    "        if user_id not in self.user_graph:\n",
    "            return np.array([0, 0, 0, 0])  # Default features for unseen users\n",
    "        \n",
    "        try:\n",
    "            degree = self.user_graph.degree(user_id)\n",
    "            clustering = nx.clustering(self.user_graph, user_id)\n",
    "            \n",
    "            # Count relation types\n",
    "            follows_count = 0\n",
    "            co_tx_count = 0\n",
    "            for neighbor in self.user_graph.neighbors(user_id):\n",
    "                edge_data = self.user_graph.get_edge_data(user_id, neighbor)\n",
    "                rel_type = edge_data.get('relation_type', 'unknown')\n",
    "                if rel_type == 'follows':\n",
    "                    follows_count += 1\n",
    "                elif rel_type == 'co_tx':\n",
    "                    co_tx_count += 1\n",
    "            \n",
    "            return np.array([degree, clustering, follows_count, co_tx_count])\n",
    "        except:\n",
    "            return np.array([0, 0, 0, 0])\n",
    "\n",
    "class OnlineLearningDataset(Dataset):\n",
    "    \"\"\"Dataset that can handle unseen data dynamically\"\"\"\n",
    "    \n",
    "    def __init__(self, interactions_df, users_df, assets_df, asset_graph_df, user_graph_df):\n",
    "        self.interactions_df = interactions_df\n",
    "        self.feature_processor = DynamicFeatureProcessor()\n",
    "        self.graph_builder = DynamicGraphBuilder()\n",
    "        \n",
    "        # Initialize with existing data\n",
    "        self.initialize_with_data(users_df, assets_df, asset_graph_df, user_graph_df)\n",
    "        \n",
    "    def initialize_with_data(self, users_df, assets_df, asset_graph_df, user_graph_df):\n",
    "        \"\"\"Initialize with existing datasets\"\"\"\n",
    "        self.feature_processor.fit(users_df, assets_df)\n",
    "        self.graph_builder.update_asset_graph(asset_graph_df)\n",
    "        self.graph_builder.update_user_graph(user_graph_df)\n",
    "        \n",
    "        # Store base features\n",
    "        self.base_user_features = {}\n",
    "        self.base_asset_features = {}\n",
    "        \n",
    "        for _, user_row in users_df.iterrows():\n",
    "            user_id = user_row['user_id']\n",
    "            self.base_user_features[user_id] = self.feature_processor.transform_user(user_row.to_dict())\n",
    "            \n",
    "        for _, asset_row in assets_df.iterrows():\n",
    "            asset_id = asset_row['asset_id']\n",
    "            self.base_asset_features[asset_id] = self.feature_processor.transform_asset(asset_row.to_dict())\n",
    "    \n",
    "    def add_new_interaction(self, user_id, asset_id, rating, tx_count, clicked):\n",
    "        \"\"\"Add new interaction dynamically\"\"\"\n",
    "        new_interaction = {\n",
    "            'user_id': user_id,\n",
    "            'asset_id': asset_id,\n",
    "            'rating': rating,\n",
    "            'tx_count': tx_count,\n",
    "            'clicked': clicked,\n",
    "            'last_interaction': pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "        }\n",
    "        \n",
    "        # Add to interactions dataframe\n",
    "        new_df = pd.DataFrame([new_interaction])\n",
    "        self.interactions_df = pd.concat([self.interactions_df, new_df], ignore_index=True)\n",
    "    \n",
    "    def add_new_user(self, user_data):\n",
    "        \"\"\"Add new user dynamically\"\"\"\n",
    "        user_id = user_data['user_id']\n",
    "        self.base_user_features[user_id] = self.feature_processor.transform_user(user_data)\n",
    "    \n",
    "    def add_new_asset(self, asset_data):\n",
    "        \"\"\"Add new asset dynamically\"\"\"\n",
    "        asset_id = asset_data['asset_id']\n",
    "        self.base_asset_features[asset_id] = self.feature_processor.transform_asset(asset_data)\n",
    "    \n",
    "    def update_graphs(self, new_asset_graph_df=None, new_user_graph_df=None):\n",
    "        \"\"\"Update graphs with new relationships\"\"\"\n",
    "        if new_asset_graph_df is not None:\n",
    "            self.graph_builder.update_asset_graph(new_asset_graph_df)\n",
    "        if new_user_graph_df is not None:\n",
    "            self.graph_builder.update_user_graph(new_user_graph_df)\n",
    "    \n",
    "    def get_user_features(self, user_id):\n",
    "        \"\"\"Get features for user (handles unseen users)\"\"\"\n",
    "        if user_id in self.base_user_features:\n",
    "            base_features = self.base_user_features[user_id]\n",
    "        else:\n",
    "            # Create default features for unseen user\n",
    "            default_user = {\n",
    "                'wallet_age_days': 100,\n",
    "                'tx_count': 50,\n",
    "                'avg_gas_used': 0.05,\n",
    "                'portfolio_diversity': 0.5,\n",
    "                'risk_score': 0.5,\n",
    "                'preferred_network': 'ethereum'\n",
    "            }\n",
    "            base_features = self.feature_processor.transform_user(default_user)\n",
    "        \n",
    "        graph_features = self.graph_builder.get_user_graph_features(user_id)\n",
    "        return np.concatenate([base_features, graph_features])\n",
    "    \n",
    "    def get_asset_features(self, asset_id):\n",
    "        \"\"\"Get features for asset (handles unseen assets)\"\"\"\n",
    "        if asset_id in self.base_asset_features:\n",
    "            base_features = self.base_asset_features[asset_id]\n",
    "        else:\n",
    "            # Create default features for unseen asset\n",
    "            default_asset = {\n",
    "                'price': 1000,\n",
    "                'volatility': 0.2,\n",
    "                'tvl': 50,\n",
    "                'audit_score': 0.7,\n",
    "                'github_commits': 500,\n",
    "                'social_sentiment': 0.5,\n",
    "                'asset_type': 'token'\n",
    "            }\n",
    "            base_features = self.feature_processor.transform_asset(default_asset)\n",
    "        \n",
    "        graph_features = self.graph_builder.get_asset_graph_features(asset_id)\n",
    "        return np.concatenate([base_features, graph_features])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.interactions_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.interactions_df.iloc[idx]\n",
    "        \n",
    "        user_id = row['user_id']\n",
    "        asset_id = row['asset_id']\n",
    "        rating = row['rating'] / 5.0  # Normalize\n",
    "        \n",
    "        user_features = self.get_user_features(user_id)\n",
    "        asset_features = self.get_asset_features(asset_id)\n",
    "        \n",
    "        user_encoded = self.feature_processor.get_user_id_encoded(user_id)\n",
    "        asset_encoded = self.feature_processor.get_asset_id_encoded(asset_id)\n",
    "        \n",
    "        return (\n",
    "            torch.LongTensor([user_encoded]),\n",
    "            torch.LongTensor([asset_encoded]),\n",
    "            torch.FloatTensor(user_features),\n",
    "            torch.FloatTensor(asset_features),\n",
    "            torch.FloatTensor([rating])\n",
    "        )\n",
    "\n",
    "class AdaptiveHybridRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, user_features_dim, item_features_dim,\n",
    "                 embedding_dim=64, hidden_dim=128, dropout_rate=0.3):\n",
    "        super(AdaptiveHybridRecommender, self).__init__()\n",
    "        \n",
    "        # Use larger embedding dimensions to handle unseen data\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embedding layers with padding for unseen data\n",
    "        self.user_embedding = nn.Embedding(num_users + 1000, embedding_dim)  # Buffer for unseen\n",
    "        self.item_embedding = nn.Embedding(num_items + 1000, embedding_dim)  # Buffer for unseen\n",
    "        \n",
    "        # Feature encoders\n",
    "        self.user_encoder = nn.Sequential(\n",
    "            nn.Linear(user_features_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.item_encoder = nn.Sequential(\n",
    "            nn.Linear(item_features_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads=4, dropout=dropout_rate)\n",
    "        \n",
    "        # Fusion network\n",
    "        self.fusion_network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 4, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.user_bn = nn.BatchNorm1d(embedding_dim)\n",
    "        self.item_bn = nn.BatchNorm1d(embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        \n",
    "        for layer in [self.user_encoder, self.item_encoder, self.fusion_network]:\n",
    "            if isinstance(layer, nn.Sequential):\n",
    "                for sublayer in layer:\n",
    "                    if isinstance(sublayer, nn.Linear):\n",
    "                        nn.init.xavier_uniform_(sublayer.weight)\n",
    "                        nn.init.zeros_(sublayer.bias)\n",
    "    \n",
    "    def forward(self, user_ids, item_ids, user_features, item_features):\n",
    "        # Handle unseen IDs by using feature-based embeddings primarily\n",
    "        user_embed = self.user_embedding(user_ids).squeeze(1)\n",
    "        item_embed = self.item_embedding(item_ids).squeeze(1)\n",
    "        \n",
    "        # Content-based features (primary for unseen data)\n",
    "        user_content = self.user_encoder(user_features)\n",
    "        item_content = self.item_encoder(item_features)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        user_embed = self.user_bn(user_embed)\n",
    "        item_embed = self.item_bn(item_embed)\n",
    "        user_content = self.user_bn(user_content)\n",
    "        item_content = self.item_bn(item_content)\n",
    "        \n",
    "        # Use attention to combine embeddings and content\n",
    "        user_combined = torch.stack([user_embed, user_content], dim=0)\n",
    "        item_combined = torch.stack([item_embed, item_content], dim=0)\n",
    "        \n",
    "        user_attended, _ = self.attention(user_combined, user_combined, user_combined)\n",
    "        item_attended, _ = self.attention(item_combined, item_combined, item_combined)\n",
    "        \n",
    "        user_attended = user_attended.mean(dim=0)\n",
    "        item_attended = item_attended.mean(dim=0)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined = torch.cat([user_attended, item_attended, user_embed, item_embed], dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.fusion_network(combined)\n",
    "        return torch.sigmoid(prediction.squeeze())\n",
    "\n",
    "class DynamicBlockchainRecommender:\n",
    "    def __init__(self, interactions_df, users_df, assets_df, asset_graph_df, user_graph_df):\n",
    "        self.dataset = OnlineLearningDataset(\n",
    "            interactions_df, users_df, assets_df, asset_graph_df, user_graph_df\n",
    "        )\n",
    "        \n",
    "        # Model dimensions\n",
    "        self.num_users = len(users_df) + 1000  # Buffer for unseen\n",
    "        self.num_items = len(assets_df) + 1000  # Buffer for unseen\n",
    "        \n",
    "        # Get feature dimensions from sample\n",
    "        sample_user_features = self.dataset.get_user_features(users_df['user_id'].iloc[0])\n",
    "        sample_asset_features = self.dataset.get_asset_features(assets_df['asset_id'].iloc[0])\n",
    "        \n",
    "        self.user_features_dim = len(sample_user_features)\n",
    "        self.item_features_dim = len(sample_asset_features)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = AdaptiveHybridRecommender(\n",
    "            self.num_users, self.num_items, \n",
    "            self.user_features_dim, self.item_features_dim\n",
    "        )\n",
    "        \n",
    "        # Online learning components\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "        # Cold start handler\n",
    "        self.cold_start_handler = EnhancedColdStartHandler()\n",
    "        self.prepare_cold_start_handler()\n",
    "        \n",
    "    def prepare_cold_start_handler(self):\n",
    "        \"\"\"Prepare cold start handler with current data\"\"\"\n",
    "        # Extract features for all known users and assets\n",
    "        user_features_list = []\n",
    "        asset_features_list = []\n",
    "        \n",
    "        for user_id in self.dataset.base_user_features.keys():\n",
    "            user_features_list.append(self.dataset.get_user_features(user_id))\n",
    "        \n",
    "        for asset_id in self.dataset.base_asset_features.keys():\n",
    "            asset_features_list.append(self.dataset.get_asset_features(asset_id))\n",
    "        \n",
    "        user_features = np.array(user_features_list)\n",
    "        asset_features = np.array(asset_features_list)\n",
    "        \n",
    "        self.cold_start_handler.fit(user_features, asset_features, self.dataset.interactions_df)\n",
    "    \n",
    "    def train(self, epochs=10, batch_size=64, online_learning=True):\n",
    "        \"\"\"Train with optional online learning\"\"\"\n",
    "        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch_idx, (user_ids, item_ids, user_features, item_features, ratings) in enumerate(dataloader):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                predictions = self.model(user_ids, item_ids, user_features, item_features)\n",
    "                loss = self.criterion(predictions, ratings)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{epochs} | Batch {batch_idx} | Loss: {loss.item():.6f}')\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f'Epoch {epoch+1}/{epochs} | Average Loss: {avg_loss:.6f}')\n",
    "            \n",
    "            # Online learning: Update cold start handler\n",
    "            if online_learning:\n",
    "                self.prepare_cold_start_handler()\n",
    "    \n",
    "    def online_update(self, new_interactions, new_users=None, new_assets=None, \n",
    "                    new_asset_graph=None, new_user_graph=None):\n",
    "        \"\"\"Update model with new data online\"\"\"\n",
    "        # Add new data to dataset\n",
    "        for interaction in new_interactions:\n",
    "            self.dataset.add_new_interaction(**interaction)\n",
    "        \n",
    "        if new_users:\n",
    "            for user_data in new_users:\n",
    "                self.dataset.add_new_user(user_data)\n",
    "        \n",
    "        if new_assets:\n",
    "            for asset_data in new_assets:\n",
    "                self.dataset.add_new_asset(asset_data)\n",
    "        \n",
    "        if new_asset_graph is not None:\n",
    "            self.dataset.update_graphs(new_asset_graph_df=new_asset_graph)\n",
    "        \n",
    "        if new_user_graph is not None:\n",
    "            self.dataset.update_graphs(new_user_graph_df=new_user_graph)\n",
    "        \n",
    "        # Retrain with new data\n",
    "        self.train(epochs=3, batch_size=32, online_learning=True)\n",
    "    \n",
    "    def predict(self, user_id, asset_id):\n",
    "        \"\"\"Predict rating for any user-asset pair (handles unseen data)\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        user_features = self.dataset.get_user_features(user_id)\n",
    "        asset_features = self.dataset.get_asset_features(asset_id)\n",
    "        \n",
    "        user_encoded = self.dataset.feature_processor.get_user_id_encoded(user_id)\n",
    "        asset_encoded = self.dataset.feature_processor.get_asset_id_encoded(asset_id)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(\n",
    "                torch.LongTensor([user_encoded]),\n",
    "                torch.LongTensor([asset_encoded]),\n",
    "                torch.FloatTensor(user_features).unsqueeze(0),\n",
    "                torch.FloatTensor(asset_features).unsqueeze(0)\n",
    "            )\n",
    "        \n",
    "        return prediction.item() * 5.0\n",
    "    \n",
    "    def recommend_top_n(self, user_id, n=10, include_unseen=True):\n",
    "        \"\"\"Generate recommendations for any user\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        user_features = self.dataset.get_user_features(user_id)\n",
    "        user_encoded = self.dataset.feature_processor.get_user_id_encoded(user_id)\n",
    "        \n",
    "        all_scores = []\n",
    "        all_assets = []\n",
    "        \n",
    "        # Consider all known assets plus some synthetic ones for diversity\n",
    "        assets_to_consider = list(self.dataset.base_asset_features.keys())\n",
    "        \n",
    "        if include_unseen:\n",
    "            # Add some synthetic asset IDs for exploration\n",
    "            synthetic_assets = [f\"ASSET_{i}\" for i in range(1000, 1020)]\n",
    "            assets_to_consider.extend(synthetic_assets)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for asset_id in assets_to_consider:\n",
    "                asset_features = self.dataset.get_asset_features(asset_id)\n",
    "                asset_encoded = self.dataset.feature_processor.get_asset_id_encoded(asset_id)\n",
    "                \n",
    "                score = self.model(\n",
    "                    torch.LongTensor([user_encoded]),\n",
    "                    torch.LongTensor([asset_encoded]),\n",
    "                    torch.FloatTensor(user_features).unsqueeze(0),\n",
    "                    torch.FloatTensor(asset_features).unsqueeze(0)\n",
    "                )\n",
    "                \n",
    "                all_scores.append(score.item())\n",
    "                all_assets.append(asset_id)\n",
    "        \n",
    "        # Get top-N recommendations\n",
    "        recommendations = sorted(zip(all_assets, all_scores), key=lambda x: x[1], reverse=True)[:n]\n",
    "        return recommendations\n",
    "    \n",
    "    def handle_unseen_user(self, user_features, n_recommendations=10):\n",
    "        \"\"\"Handle completely unseen users\"\"\"\n",
    "        return self.cold_start_handler.recommend_for_cold_start_user(user_features, n_recommendations)\n",
    "    \n",
    "    def handle_unseen_asset(self, asset_features, n_recommendations=10):\n",
    "        \"\"\"Handle completely unseen assets\"\"\"\n",
    "        return self.cold_start_handler.recommend_for_cold_start_item(asset_features, n_recommendations)\n",
    "\n",
    "# Enhanced Cold Start Handler (from previous implementation)\n",
    "class EnhancedColdStartHandler:\n",
    "    def __init__(self, n_clusters=5, n_neighbors=10):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.user_cluster_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.item_cluster_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.user_knn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        self.item_knn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, user_features, item_features, interactions_df):\n",
    "        self.user_cluster_model.fit(user_features)\n",
    "        self.item_cluster_model.fit(item_features)\n",
    "        self.user_knn.fit(user_features)\n",
    "        self.item_knn.fit(item_features)\n",
    "        \n",
    "        self.user_item_matrix = interactions_df.pivot(\n",
    "            index='user_id_encoded', \n",
    "            columns='asset_id_encoded', \n",
    "            values='rating'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def get_similar_users(self, user_features, n_neighbors=5):\n",
    "        distances, indices = self.user_knn.kneighbors(user_features.reshape(1, -1), n_neighbors=n_neighbors)\n",
    "        return indices[0], distances[0]\n",
    "    \n",
    "    def get_similar_items(self, item_features, n_neighbors=5):\n",
    "        distances, indices = self.item_knn.kneighbors(item_features.reshape(1, -1), n_neighbors=n_neighbors)\n",
    "        return indices[0], distances[0]\n",
    "    \n",
    "    def recommend_for_cold_start_user(self, new_user_features, n_recommendations=10):\n",
    "        similar_users, _ = self.get_similar_users(new_user_features)\n",
    "        similar_users_ratings = self.user_item_matrix.iloc[similar_users]\n",
    "        item_scores = similar_users_ratings.mean(axis=0)\n",
    "        top_items = item_scores.nlargest(n_recommendations)\n",
    "        return top_items.index.values, top_items.values\n",
    "    \n",
    "    def recommend_for_cold_start_item(self, new_item_features, n_recommendations=10):\n",
    "        similar_items, _ = self.get_similar_items(new_item_features)\n",
    "        similar_items_ratings = self.user_item_matrix.iloc[:, similar_items]\n",
    "        user_scores = similar_items_ratings.mean(axis=1)\n",
    "        top_users = user_scores.nlargest(n_recommendations)\n",
    "        return top_users.index.values, top_users.values\n",
    "\n",
    "# Usage Example\n",
    "def main():\n",
    "    # Load datasets\n",
    "    interactions_df = pd.read_csv('interactions.csv')\n",
    "    users_df = pd.read_csv('users.csv')\n",
    "    assets_df = pd.read_csv('assets.csv')\n",
    "    asset_graph_df = pd.read_csv('asset_graph.csv')\n",
    "    user_graph_df = pd.read_csv('user_graph.csv')\n",
    "    \n",
    "    # Initialize dynamic recommender\n",
    "    recommender = DynamicBlockchainRecommender(\n",
    "        interactions_df, users_df, assets_df, asset_graph_df, user_graph_df\n",
    "    )\n",
    "    \n",
    "    print(\"Training initial model...\")\n",
    "    recommender.train(epochs=5)\n",
    "    \n",
    "    # Test with existing data\n",
    "    print(\"\\n=== Testing with Existing Data ===\")\n",
    "    recs = recommender.recommend_top_n(\"U1\", n=5)\n",
    "    print(\"Recommendations for U1:\", recs)\n",
    "    \n",
    "    # Test with unseen user\n",
    "    print(\"\\n=== Testing with Unseen User ===\")\n",
    "    unseen_user_recs = recommender.recommend_top_n(\"UNSEEN_USER_123\", n=5)\n",
    "    print(\"Recommendations for unseen user:\", unseen_user_recs)\n",
    "    \n",
    "    # Test with unseen asset\n",
    "    print(\"\\n=== Testing Prediction with Unseen Asset ===\")\n",
    "    prediction = recommender.predict(\"U1\", \"UNSEEN_ASSET_456\")\n",
    "    print(f\"Prediction for U1-UNSEEN_ASSET_456: {prediction:.2f}\")\n",
    "    \n",
    "    # Online learning example\n",
    "    print(\"\\n=== Online Learning Example ===\")\n",
    "    new_interactions = [\n",
    "        {'user_id': 'U1', 'asset_id': 'ASSET_NEW', 'rating': 4.5, 'tx_count': 1, 'clicked': 1},\n",
    "        {'user_id': 'UNSEEN_USER_999', 'asset_id': 'ASSET1', 'rating': 3.8, 'tx_count': 1, 'clicked': 1}\n",
    "    ]\n",
    "    \n",
    "    new_users = [{\n",
    "        'user_id': 'UNSEEN_USER_999',\n",
    "        'wallet_age_days': 200,\n",
    "        'tx_count': 150,\n",
    "        'avg_gas_used': 0.04,\n",
    "        'portfolio_diversity': 0.7,\n",
    "        'risk_score': 0.6,\n",
    "        'preferred_network': 'solana'\n",
    "    }]\n",
    "    \n",
    "    new_assets = [{\n",
    "        'asset_id': 'ASSET_NEW',\n",
    "        'asset_type': 'defi',\n",
    "        'price': 2500,\n",
    "        'volatility': 0.18,\n",
    "        'tvl': 80,\n",
    "        'audit_score': 0.9,\n",
    "        'github_commits': 1200,\n",
    "        'social_sentiment': 0.8\n",
    "    }]\n",
    "    \n",
    "    recommender.online_update(\n",
    "        new_interactions=new_interactions,\n",
    "        new_users=new_users,\n",
    "        new_assets=new_assets\n",
    "    )\n",
    "    \n",
    "    print(\"Online learning completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "## Key Features for Handling Unseen Data:\n",
    "\n",
    "### 1. **Dynamic Feature Processing**\n",
    "- **Adaptive Encoders**: Handle unseen categorical values gracefully\n",
    "- **Default Feature Generation**: Create reasonable defaults for unseen users/assets\n",
    "- **Feature Buffers**: Allow embedding layers to handle new IDs\n",
    "\n",
    "### 2. **Online Learning Capabilities**\n",
    "- **Incremental Updates**: Add new data without retraining from scratch\n",
    "- **Dynamic Dataset**: Continuously updated with new interactions\n",
    "- **Graph Updates**: Incorporate new relationships in real-time\n",
    "\n",
    "### 3. **Robust Cold Start Handling**\n",
    "- **Feature-Based Similarity**: Use content features for completely new entities\n",
    "- **Fallback Strategies**: Default to content-based recommendations when collaborative data is unavailable\n",
    "- **Exploration**: Include synthetic items for diversity\n",
    "\n",
    "### 4. **Flexible Prediction Interface**\n",
    "```python\n",
    "# Works with any combination of users and assets\n",
    "recommender.predict(\"BRAND_NEW_USER\", \"BRAND_NEW_ASSET\")\n",
    "\n",
    "# Handles mixed scenarios\n",
    "recommender.predict(\"EXISTING_USER\", \"NEW_ASSET\")\n",
    "recommender.predict(\"NEW_USER\", \"EXISTING_ASSET\")\n",
    "```\n",
    "\n",
    "### 5. **Real-time Updates**\n",
    "```python\n",
    "# Add new interactions\n",
    "recommender.online_update(new_interactions=[...])\n",
    "\n",
    "# Add new users/assets\n",
    "recommender.online_update(new_users=[...], new_assets=[...])\n",
    "\n",
    "# Update relationship graphs\n",
    "recommender.online_update(new_asset_graph=new_relationships)\n",
    "```\n",
    "\n",
    "This system can now handle:\n",
    "- **Completely new users** with no interaction history\n",
    "- **Completely new assets** with no user interactions  \n",
    "- **New relationship types** in graphs\n",
    "- **Real-time data updates** without full retraining\n",
    "- **Mixed scenarios** with any combination of known/unknown entities\n",
    "\n",
    "The model gracefully degrades to content-based and graph-based features when collaborative data is unavailable, ensuring robust performance even with completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f50a4-22fc-442f-8ecb-0ebf4ddfc446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
